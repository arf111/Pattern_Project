{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K1livok-jjlT"
   },
   "outputs": [],
   "source": [
    "url='http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# Preamble\n",
    "# ========\n",
    "# \n",
    "# This assignment is meant to provide you with an opportunity to create your first machine learning algorithm, and begin exploring the relationship between the answers the algorithm provides and the structure of the data.\n",
    "# \n",
    "# Attached to this assignment is a dataset containing 4 columns, x (a feature), y (another feature) TL (the true label of the class) and L (the known label of the class). Typically with machine learning, you would not have access to TL for all datapoint or you would not need to build a classifier, but it is provided here as a learning exercise. You should not use the TL column during training, but you can use it after training to see how the structure of the data interacts with the algorithm to produce answers that are correct or incorrect. Most of the data in the L column is empty (NaN in scipy). Those cells which have a label are your labels training data. Those cells which do not have a label are the data that you need to classify.\n",
    "# \n",
    "# You will be building, from first principles a K- nearest neighbour classifier in Python. You cannot use open-source code which implements this algorithm, that is the second step of your assignment. You are encouraged to use the lower-level components of SciPy, such as Pandas data frames to speed up your code, and save you the work of implementing low-level data management tasks.\n",
    "# \n",
    "# Assignment\n",
    "# ==========\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import operator\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score  \n",
    "from sklearn.metrics import confusion_matrix  \n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# Loading and preprocessing the dataset\n",
    "\n",
    "#dataFrame = pd.read_csv('knnDataSetCopy.csv')\n",
    "dataFrame = pd.read_csv(url)\n",
    "print (dataFrame.head())\n",
    "dataFrame.columns = ['no','x','y','TL','L']\n",
    "###################################################################################\n",
    "x=0\n",
    "if x==1:\n",
    "  Set1 = dataFrame.drop('no', axis=1)\n",
    "  global outputCheck\n",
    "  # Preparing Test Data\n",
    "  f = Set1[~Set1.L.notnull()]\n",
    "  g = f.values\n",
    "  removeNon = g[:, :-1]  # With true label\n",
    "  testSet11 = removeNon\n",
    "  testData = removeNon[:, :-1]  # without true label\n",
    "  #print(testData)  # test set\n",
    "\n",
    "  # Preparing Training Data Set\n",
    "  E = Set1.dropna(axis=0, how='any', thresh=None, subset=None)  # STEP 3\n",
    "  k = E.values\n",
    "  trainData = k[:, :-1]\n",
    "  print(type(trainData))\n",
    "\n",
    "##################################################################################\n",
    "#Set1=dataFrame.drop('TL',axis=1)\n",
    "Set1 = dataFrame.drop(['no','TL'], axis=1)\n",
    "\n",
    "\n",
    "Set1.L[Set1.L == 'Iris-setosa'] = 1\n",
    "Set1.L[Set1.L == 'Iris-versicolor'] = 2\n",
    "Set1.L[Set1.L == 'Iris-virginica'] = 3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "msk = np.random.rand(len(Set1)) < 0.8\n",
    "E = Set1[msk]\n",
    "k=E.values\n",
    "trainData=k\n",
    "print(type(trainData))\n",
    "f = Set1[~msk]\n",
    "g=f.values\n",
    "\n",
    "removeNon=g\n",
    "\n",
    "\n",
    "testSet11=removeNon\n",
    "testData=testSet11[:,:-1]\n",
    "#testData=testData.values\n",
    "\n",
    "# print ('train',len(train))\n",
    "# print('test',len(test))\n",
    "\n",
    " \n",
    "\n",
    "#print('printing trainig data',trainData)  # training set\n",
    "\n",
    "\n",
    "# 2) Implement the k-nearest neighbour algorithm in Python. Just use simple data structures, the small datasets we are employing will not require kD-tree optimization.\n",
    "#Your algorithm should take a labeled dataset (3 column table), and unlabeled dataset (2 column table), the parameter k, and a boolean value feedback_classification.\n",
    "#The parameter k is the number of neighbours to consider when classifying. The boolean feedback_classification parameter is used to set the behaviour of the classifier.\n",
    "#When it is true, previously unlabeled data that has been classified becomes part of the training set. When it is false, only data in the training set is used for nearest\n",
    "#neighbour determination for all unlabeled data. (20 marks)\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "#def knn(trainSet, xTest, K, feedback_classification):\n",
    "    #pass\n",
    "\n",
    "def similarSet (trainingSet, testInstance, k):\n",
    "    distance = []\n",
    "    for x in range(len(trainingSet)):\n",
    "        teuclid = sum(pow(trainingSet[x, 0:2] - testInstance,2))\n",
    "        dist = np.sqrt(teuclid)\n",
    "        distance.append((trainingSet[x].tolist(), dist))\n",
    "    distance.sort(key=operator.itemgetter(1))\n",
    "    neighbors = []\n",
    "    for x in range(k):\n",
    "        neighbors.append(distance[x][0])\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "def response(neighbors):\n",
    "    a=0\n",
    "    b=0\n",
    "    c=0\n",
    "    for i in neighbors:\n",
    "      if i[2]==1:\n",
    "        a=a+1\n",
    "      elif i[2]==2:\n",
    "        b=b+1\n",
    "      else:\n",
    "        c=c+1\n",
    "     \n",
    "    #print ('printing ',a,b,c)\n",
    "    \n",
    "    if a>=b and b>=c:\n",
    "      return 1\n",
    "    \n",
    "    elif b>=a and a>=c:\n",
    "      return 2\n",
    "    elif c>=a and  a>=b :\n",
    "      return 3\n",
    "\n",
    "def accuracyFunction(testSet, prediction):\n",
    "    #print ('len printing',len(testSet),len(prediction))\n",
    "    z=[i for i, j in zip(testSet, prediction) if i == j]\n",
    "    return (len(z)/len(testSet))\n",
    "\n",
    "def knn(trainData, testData, k, feedback_classification):\n",
    "    predictions = []\n",
    "    print ('knn called')\n",
    "    #print(trainData)\n",
    "    print ('-------------------')\n",
    "    #print (testData)\n",
    "    for i in range(len(testData)):\n",
    "        neighbours = similarSet(trainData,testData[i],k)\n",
    "        #print(neighbours,'is printing')\n",
    "        result = response(neighbours)\n",
    "        #print(result)\n",
    "        predictions.append(result)\n",
    "        newV = ([[testData[i, 0], testData[i, 1], result]])\n",
    "        if feedback_classification == True:\n",
    "            trainData = np.append(trainData, newV, axis=0)\n",
    "        #print(trainData)\n",
    "\n",
    "    #print(neighbors)\n",
    "    #print(result)\n",
    "    set111=testSet11[:, 2]\n",
    "    \n",
    "    accuracy = accuracyFunction(set111, predictions)\n",
    "    print('the accurecy= ',accuracy*100,'%')\n",
    "    #USE AFTER CLASSIFICATION\n",
    "    plt.scatter(trainData[:, 0], trainData[:, 1], c=trainData[:, 2], label='TrainDataSet', s=70, alpha=0.7, marker='.')\n",
    "    #plt.scatter(removeNon[:, 0], removeNon[:, 1], c=predictions, label='TestingDataSet', s=50, alpha=0.5, marker='x')\n",
    "    # ON WHEN RANDOM NEEDED\n",
    "    plt.scatter(testSet11[:, 0], testSet11[:, 1], c=predictions, label='TestingDataSet', s=50, alpha=0.5, marker='x')\n",
    "    plt.xlabel('Feature value of x',fontsize=12)\n",
    "    plt.ylabel('Feature value of y',fontsize=12)\n",
    "    plt.title('After classification', fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"main function\")\n",
    "    global trainData\n",
    "# random check\n",
    "    #print(testData)3\n",
    "    #print(trainData)\n",
    "    #print(testSet11)\n",
    "    #print(len(testData))\n",
    "    #np.random.shuffle(testSet11)# ON WHEN RANDOM NEEDED\n",
    "    #print(df2)\n",
    "    #randTestData = testSet11[:, :-1]# ON WHEN RANDOM NEEDED\n",
    "    #print(testSet11)\n",
    "    #print(randTestData)\n",
    "# end of random check\n",
    "    \"\"\"\n",
    "    #for randomization for randomization for randomization for randomization\n",
    "    testSet11Copy=testSet11\n",
    "    np.random.shuffle(testSet11Copy)\n",
    "    print(testSet11Copy)\n",
    "    testDataRand = testSet11Copy[:, :-1]\n",
    "    print(testDataRand)\n",
    "    \"\"\"\n",
    "\n",
    "    k = 3\n",
    "    feedback_classification = False\n",
    "    predictions = []\n",
    "\n",
    "\n",
    "    print(feedback_classification)\n",
    "    # Main KNN\n",
    "    knn(trainData, testData, k, feedback_classification)\n",
    "\n",
    "    \n",
    "main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "asignment_1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
